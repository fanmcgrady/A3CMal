# encoding=utf-8
import csv
import hashlib
import os
import random
from collections import OrderedDict

import gym
import numpy as np
from gym import spaces

from controls import manipulate2 as manipulate
from env import pefeatures
from tools import interface

ACTION_LOOKUP = {i: act for i, act in enumerate(
    manipulate.ACTION_TABLE.keys())}

# change this to function to the AV engine to attack
# function should be of the form
# def label_function( bytez ):
#    # returns 0.0 if benign else 1.0 if malware
label_function = interface.get_label_local


# an environment must define its
# observation space and action space
# and have at least two methods: reset and step.

# * env.reset will reset the environment to the initial state and return the initial observation.
# * env.step will execute a given action, move to the next state and return four values:
#   a next observation
#   a scalar reward
#   a boolean value indicating whether the current state is terminal or not
#   additional information
# * env.render will render the current state.
class MalwareEnv(gym.Env):
    metadata = {'render.modes': ['human']}

    def __init__(self, sha256list, random_sample=True, maxturns=3, output_path='evaded/blackbox/', cache=True,
                 test=False):
        # PCA部分
        # features, nor_features, U, S, V, scale_, min_, pca_component = self.load_PCA_model()
        # self.PCA_V = V
        # self.feature_scale_ = scale_
        # self.feature_min_ = min_
        # self.PCA_component = pca_component

        self.total_turn = 0
        self.test_turn = 0
        self.episode = -1  # 共训练了多少轮
        self.cache = cache
        self.available_sha256 = sha256list
        self.action_space = spaces.Discrete(len(ACTION_LOOKUP))
        self.maxturns = maxturns
        self.feature_extractor = pefeatures.PEFeatureExtractor()
        self.random_sample = random_sample
        self.sample_iteration_index = 0
        self.test = test
        self.output_path = os.path.join(
            os.path.dirname(
                os.path.dirname(
                    os.path.dirname(
                        os.path.abspath(__file__)))), output_path)

        self.original_path = os.path.join(
            os.path.dirname(
                os.path.dirname(
                    os.path.dirname(
                        os.path.abspath(__file__)))), "Sample/original")
        self.modification_path = os.path.join(
            os.path.dirname(
                os.path.dirname(
                    os.path.dirname(
                        os.path.abspath(__file__)))), "Sample/modification/")

        self.samples_path = os.path.join(
            os.path.dirname(
                os.path.dirname(
                    os.path.dirname(
                        os.path.abspath(__file__)))), "Sample/")

        if not os.path.exists(output_path):
            os.makedirs(output_path)

        self.history = OrderedDict()
        self.current_reward = 0

        self.samples = {}
        if self.cache:
            for sha256 in self.available_sha256:
                try:
                    self.samples[sha256] = interface.fetch_file(sha256)
                    # self.samples[sha256] = interface.fetch_file(sha256, self.test)
                except interface.FileRetrievalFailure:
                    print("failed fetching file")
                    continue  # try a new sha256...this one can't be retrieved from storage

        self._reset()

    def _step(self, action_index):
        self.total_turn += 1
        self.turns += 1
        self._take_action(action_index)  # update self.bytez
        # get reward
        try:
            self.label = label_function(self.bytez)
        except interface.ClassificationFailure:
            print("Failed to classify file")
            episode_over = True
        else:
            # self.observation_space = self.feature_extractor.extract2(self.bytez)
            # PCA
            self.observation_space = self.compute_observation(self.bytez)

            if self.label == 0:
                # we win!
                # reward = 100 - (self.turns - 1) / 60  # !! a strong reward
                reward = 10
                episode_over = True
                self.history[self.sha256]['evaded'] = True

                # store sample to output directory
                if self.test:
                    m = hashlib.sha256()
                    m.update(self.bytez)
                    sha256 = m.hexdigest()
                    self.history[self.sha256]['evaded_sha256'] = sha256

                    with open(os.path.join(self.modification_path, str(self.sample_iteration_index-2)+self.sha256), 'wb') as outfile:
                        outfile.write(self.bytez)

                with open(os.path.join(self.samples_path, 'successful_actions.txt'), 'a+') as f:
                    print("{}:{}".format(self.sha256, self.history[self.sha256]['actions']))

            elif self.turns >= self.maxturns:
                # out of turns :(
                reward = 0.0
                episode_over = True
            else:
                reward = 0.0
                episode_over = False

        # if episode_over:
        #     print("episode is over: reward = {}!".format(reward))

        self.current_reward = reward

        return self.observation_space, reward, episode_over, {}

    def _take_action(self, action_index):
        assert action_index < len(ACTION_LOOKUP)
        action = ACTION_LOOKUP[action_index]
        self.history[self.sha256]['actions'].append(action)
        self.bytez = bytes(manipulate.modify_without_breaking(self.bytez, [action]))

        # print("turns {} : {}".format(self.turns, action))

    def _reset(self):
        self.turns = 0
        self.episode += 1
        while True:
            # get the new environment
            if self.random_sample:
                self.sha256 = random.choice(self.available_sha256)
            else:  # draw a sample at random
                self.sha256 = self.available_sha256[self.sample_iteration_index % len(self.available_sha256)]
                self.sample_iteration_index += 1

            self.history[self.sha256] = {'actions': [], 'evaded': False}

            if self.cache:
                self.bytez = self.samples[self.sha256]
            else:
                try:
                    self.bytez = interface.fetch_file(self.sha256, self.test)
                except interface.FileRetrievalFailure:
                    print("failed fetching file")
                    continue  # try a new sha256...this one can't be retrieved from storage

            if self.test and self.episode>0:
                with open(os.path.join(self.original_path, str(self.sample_iteration_index - 2) + self.sha256),
                          'wb') as outfile:
                    outfile.write(self.bytez)

            original_label = label_function(self.bytez)
            if original_label == 0:
                # skip this one, it's already benign, and the graduation_agent will learn nothing
                continue

            self.tips = ' ' if not self.test else 'test '

            if self.episode > 0:
                print("--------------------------------------------------------------------------------")
                print("{}episode {} select training sample: {}".format(self.tips, self.episode, self.sha256))
                if self.test:
                    with open("test_log.txt", 'a+') as f:
                        f.write("Process {} select sample: {}\n".format(self.sample_iteration_index - 2, self.sha256))
                print("--------------------------------------------------------------------------------")

            # self.observation_space = self.feature_extractor.extract2(self.bytez)
            # PCA
            self.observation_space = self.compute_observation(self.bytez)

            break  # we're done here

        return np.asarray(self.observation_space)

    def _render(self, mode='human', close=False):
        pass

    ## PCA process
    # scale features
    def scale_min_imp(self, X, scale_, min_):
        X *= scale_
        X += min_
        return X

    # read csv
    def readDictCSV(self, filename=""):
        with open(filename, 'r') as csv_file:
            reader = csv.reader(csv_file)
            mydict = dict(reader)
        return mydict

    # load PCA model
    def load_PCA_model(self):
        features = np.load("pca/features.npy")
        nor_features = np.load("pca/nor_features.npy")
        U = np.load("pca/U.npy")
        S = np.load("pca/S.npy")
        V = np.load("pca/V.npy")
        scale_ = np.load("pca/scale.npy")
        min_ = np.load("pca/min.npy")
        dic_elements = self.readDictCSV("pca/dic_elements.csv")
        pca_component = int(dic_elements['n_component'])
        return features, nor_features, U, S, V, scale_, min_, pca_component

    def compute_observation(self, bytez):
        raw_features = self.feature_extractor.extract2(bytez)
        # observation = np.dot(raw_features[np.newaxis, :], self.PCA_V.T[:, :self.PCA_component])
        # return observation[0]
        return raw_features
