# encoding=utf-8
import csv
import hashlib
import os
import random
from collections import OrderedDict

import gym
import numpy as np
from gym import spaces

from gym_malware.envs.controls import manipulate2 as manipulate
from gym_malware.envs.utils import interface, pefeatures

ACTION_LOOKUP = {i: act for i, act in enumerate(
    manipulate.ACTION_TABLE.keys())}

# change this to function to the AV engine to attack
# function should be of the form
# def label_function( bytez ):
#    # returns 0.0 if benign else 1.0 if malware
label_function = interface.get_label_local


# an environment must define its
# observation space and action space
# and have at least two methods: reset and step.

# * env.reset will reset the environment to the initial state and return the initial observation.
# * env.step will execute a given action, move to the next state and return four values:
#   a next observation
#   a scalar reward
#   a boolean value indicating whether the current state is terminal or not
#   additional information
# * env.render will render the current state.
class MalwareEnv(gym.Env):
    metadata = {'render.modes': ['human']}

    def __init__(self, sha256list, random_sample=True, maxturns=3, output_path='evaded/blackbox/', cache=True,
                 test=False):
        # PCA部分
        # features, nor_features, U, S, V, scale_, min_, pca_component = self.load_PCA_model()
        # self.PCA_V = V
        # self.feature_scale_ = scale_
        # self.feature_min_ = min_
        # self.PCA_component = pca_component

        self.total_turn = 0
        self.test_turn = 0
        self.episode = -1  # 共训练了多少轮
        self.cache = cache
        self.available_sha256 = sha256list
        self.action_space = spaces.Discrete(len(ACTION_LOOKUP))
        self.maxturns = maxturns
        self.feature_extractor = pefeatures.PEFeatureExtractor()
        self.random_sample = random_sample
        self.sample_iteration_index = 0
        self.test = test
        self.output_path = os.path.join(
            os.path.dirname(
                os.path.dirname(
                    os.path.dirname(
                        os.path.abspath(__file__)))), output_path)

        self.original_path = os.path.join(
            os.path.dirname(
                os.path.dirname(
                    os.path.dirname(
                        os.path.abspath(__file__)))), "Sample/original")
        self.modification_path = os.path.join(
            os.path.dirname(
                os.path.dirname(
                    os.path.dirname(
                        os.path.abspath(__file__)))), "Sample/modification/")

        self.samples_path = os.path.join(
            os.path.dirname(
                os.path.dirname(
                    os.path.dirname(
                        os.path.abspath(__file__)))), "Sample/")

        if not os.path.exists(output_path):
            os.makedirs(output_path)

        self.history = OrderedDict()
        self.current_reward = 0

        self.samples = {}
        if self.cache:
            for sha256 in self.available_sha256:
                try:
                    self.samples[sha256] = interface.fetch_file(sha256)
                    # self.samples[sha256] = interface.fetch_file(sha256, self.test)
                except interface.FileRetrievalFailure:
                    print("failed fetching file")
                    continue  # try a new sha256...this one can't be retrieved from storage

        self._reset()

    def _step(self, action_index):
        self.total_turn += 1
        self.turns += 1
        self._take_action(action_index)  # update self.bytez
        # get reward
        try:
            self.label = label_function(self.bytez)
        except interface.ClassificationFailure:
            print("Failed to classify file")
            episode_over = True
        else:
            # self.observation_space = self.feature_extractor.extract2(self.bytez)
            # PCA
            self.observation_space = self.compute_observation(self.bytez)

            if self.label == 0:
                # we win!
                # reward = 100 - (self.turns - 1) / 60  # !! a strong reward
                reward = 10
                episode_over = True
                self.history[self.sha256]['evaded'] = True

                # store sample to output directory
                if self.test:
                    m = hashlib.sha256()
                    m.update(self.bytez)
                    sha256 = m.hexdigest()
                    self.history[self.sha256]['evaded_sha256'] = sha256

                    with open(os.path.join(self.modification_path, str(self.sample_iteration_index-2)+self.sha256), 'wb') as outfile:
                        outfile.write(self.bytez)

                with open(os.path.join(self.samples_path, 'successful_actions.txt'), 'a+') as f:
                    print("{}:{}".format(self.sha256, self.history[self.sha256]['actions']))

            elif self.turns >= self.maxturns:
                # out of turns :(
                reward = 0.0
                episode_over = True
            else:
                reward = 0.0
                episode_over = False

        # if episode_over:
        #     print("episode is over: reward = {}!".format(reward))

        self.current_reward = reward

        return self.observation_space, reward, episode_over, {}

    def _take_action(self, action_index):
        assert action_index < len(ACTION_LOOKUP)
        action = ACTION_LOOKUP[action_index]
        self.history[self.sha256]['actions'].append(action)
        self.bytez = bytes(manipulate.modify_without_breaking(self.bytez, [action]))

        # print("turns {} : {}".format(self.turns, action))

    def _reset(self):
        self.turns = 0
        self.episode += 1
        while True:
            # get the new environment
            if self.random_sample:
                self.sha256 = random.choice(self.available_sha256)
            else:  # draw a sample at random
                self.sha256 = self.available_sha256[self.sample_iteration_index % len(self.available_sha256)]
                self.sample_iteration_index += 1

            self.history[self.sha256] = {'actions': [], 'evaded': False}

            if self.cache:
                self.bytez = self.samples[self.sha256]
            else:
                try:
                    self.bytez = interface.fetch_file(self.sha256, self.test)
                except interface.FileRetrievalFailure:
                    print("failed fetching file")
                    continue  # try a new sha256...this one can't be retrieved from storage

            if self.test and self.episode>0:
                with open(os.path.join(self.original_path, str(self.sample_iteration_index - 2) + self.sha256),
                          'wb') as outfile:
                    outfile.write(self.bytez)

            original_label = label_function(self.bytez)
            if original_label == 0:
                # skip this one, it's already benign, and the graduation_agent will learn nothing
                continue

            self.tips = ' ' if not self.test else 'test '

            if self.episode > 0:
                print("--------------------------------------------------------------------------------")
                print("{}episode {} select training sample: {}".format(self.tips, self.episode, self.sha256))
                if self.test:
                    with open("test_log.txt", 'a+') as f:
                        f.write("Process {} select sample: {}\n".format(self.sample_iteration_index - 2, self.sha256))
                print("--------------------------------------------------------------------------------")

            # self.observation_space = self.feature_extractor.extract2(self.bytez)
            # PCA
            self.observation_space = self.compute_observation(self.bytez)

            break  # we're done here

        return np.asarray(self.observation_space)

    def _render(self, mode='human', close=False):
        pass

    ## PCA process
    # scale features
    def scale_min_imp(self, X, scale_, min_):
        X *= scale_
        X += min_
        return X

    # read csv
    def readDictCSV(self, filename=""):
        with open(filename, 'r') as csv_file:
            reader = csv.reader(csv_file)
            mydict = dict(reader)
        return mydict

    # load PCA model
    def load_PCA_model(self):
        features = np.load("pca/features.npy")
        nor_features = np.load("pca/nor_features.npy")
        U = np.load("pca/U.npy")
        S = np.load("pca/S.npy")
        V = np.load("pca/V.npy")
        scale_ = np.load("pca/scale.npy")
        min_ = np.load("pca/min.npy")
        dic_elements = self.readDictCSV("pca/dic_elements.csv")
        pca_component = int(dic_elements['n_component'])
        return features, nor_features, U, S, V, scale_, min_, pca_component

    def compute_observation(self, bytez):
        raw_features = self.feature_extractor.extract2(bytez)
        # observation = np.dot(raw_features[np.newaxis, :], self.PCA_V.T[:, :self.PCA_component])
        # return observation[0]
        return raw_features
